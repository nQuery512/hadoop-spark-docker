{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T05:45:04.413090Z",
     "start_time": "2019-07-02T05:45:04.406172Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "import pandas as pd\n",
    "import re as re\n",
    "import numpy as np\n",
    "import sys, glob\n",
    "\n",
    "from sparkdl import readImages\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "sys.path.extend(glob.glob(os.path.join(os.path.expanduser(\"~\"), \".ivy2/jars/*.jar\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sparkdl in /opt/conda/envs/py35/lib/python3.5/site-packages (0.2.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/envs/py35/lib/python3.5/site-packages (6.2.1)\n",
      "Requirement already satisfied: keras in /opt/conda/envs/py35/lib/python3.5/site-packages (2.3.1)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/envs/py35/lib/python3.5/site-packages (1.14.0)\n",
      "Requirement already satisfied: tensorframes in /opt/conda/envs/py35/lib/python3.5/site-packages (0.2.9)\n",
      "Requirement already satisfied: kafka in /opt/conda/envs/py35/lib/python3.5/site-packages (1.3.5)\n",
      "Requirement already satisfied: tensorflowonspark in /opt/conda/envs/py35/lib/python3.5/site-packages (2.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/py35/lib/python3.5/site-packages (19.2)\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/py35/lib/python3.5/site-packages (0.39)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/py35/lib/python3.5/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/py35/lib/python3.5/site-packages (from keras) (5.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/py35/lib/python3.5/site-packages (from keras) (1.15.2)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/py35/lib/python3.5/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/py35/lib/python3.5/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/py35/lib/python3.5/site-packages (from keras) (1.3.3)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/envs/py35/lib/python3.5/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (1.25.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/py35/lib/python3.5/site-packages (from packaging) (2.4.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/py35/lib/python3.5/site-packages (from protobuf>=3.6.1->tensorflow) (42.0.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/py35/lib/python3.5/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sparkdl pillow keras tensorflow tensorframes kafka tensorflowonspark packaging jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info --packages databricks:0.1.0-spark2.1-s_2.11\n"
     ]
    }
   ],
   "source": [
    "os.environ['SPARK_OPTS'] = \"--driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info --packages databricks:0.1.0-spark2.1-s_2.11\"\n",
    "print(os.environ['SPARK_OPTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://816f6d3d1b41:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://54.72.254.85:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TEST</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://54.72.254.85:7077 appName=TEST>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf=pyspark.SparkConf().setAppName(\"TEST\").setMaster(\"spark://54.72.254.85:7077\")\n",
    "conf.set(\"spark.executor.memory\", \"3g\")\n",
    "conf.set(\"spark.driver.memory\", \"3g\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning With PySpark\n",
    "\n",
    "Here we'll be trying to develop a spark app that will leverage the power of deep learning for a multi class classification problem. We'll be using **Deep Learning Pipelines** which is an open source library that developed by Databricks. It's a great effort indeed that brings high level APIs for scalable deep learning in Python with Spark.\n",
    "\n",
    "We'll make this classification task on some handwritten image data. Now, there are 10 different classes in Bangla digits from 0 to 9. \n",
    "\n",
    "- Load image data on spark data-frame\n",
    "- Deploy InceptionV3 Model On Spark Cluster\n",
    "- Multi-Class Classification With Logistic Regression(multinomial) Classifier\n",
    "- Evaluate The Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T05:45:26.477793Z",
     "start_time": "2019-07-02T05:45:15.139500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://816f6d3d1b41:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://54.72.254.85:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TEST</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f19640498d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('TEST').master(\"spark://54.72.254.85:7077\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All Images\n",
    "\n",
    "Each class (from 0 to 9) contains almost 500 handwritten bangla digits. Here we manually load each images into spark data-frame with a target column. After loading the whole dataset we split into 8:2 ratio randomly. Thus create a trainig set and test set. Our goal is to train the model with the training data set and finally evaluate the model with test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o89.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, 172.18.0.4, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9e73903b97d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mzero_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/mnist/training/0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mzero_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o89.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, 172.18.0.4, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "zero_df = readImages(\"./data/mnist/training/0\").withColumn(\"label\", lit(0))\n",
    "zero_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T05:46:06.491172Z",
     "start_time": "2019-07-02T05:46:03.311498Z"
    }
   },
   "outputs": [],
   "source": [
    "# load train image\n",
    "#zero_df = readImages(\"./data/mnist/training/0\").withColumn(\"label\", lit(0))\n",
    "\n",
    "\n",
    "\n",
    "zero_df = spark.read.format(\"image\").load(\"./data/mnist/training/0\").withColumn(\"label\", lit(0))\n",
    "one_df = spark.read.format(\"image\").load(\"./data/mnist/training/1\").withColumn(\"label\", lit(1))\n",
    "two_df = spark.read.format(\"image\").load(\"./data/mnist/training/2\").withColumn(\"label\", lit(2))\n",
    "three_df = spark.read.format(\"image\").load(\"./data/mnist/training/3\").withColumn(\"label\", lit(3))\n",
    "four_df = spark.read.format(\"image\").load(\"./data/mnist/training/4\").withColumn(\"label\", lit(4))\n",
    "five_df = spark.read.format(\"image\").load(\"./data/mnist/training/5\").withColumn(\"label\", lit(5))\n",
    "six_df = spark.read.format(\"image\").load(\"./data/mnist/training/6\").withColumn(\"label\", lit(6))\n",
    "seven_df = spark.read.format(\"image\").load(\"./data/mnist/training/7\").withColumn(\"label\", lit(7))\n",
    "eight_df = spark.read.format(\"image\").load(\"./data/mnist/training/8\").withColumn(\"label\", lit(8))\n",
    "nine_df = spark.read.format(\"image\").load(\"./data/mnist/training/9\").withColumn(\"label\", lit(9))\n",
    "\n",
    "\n",
    "#zero_df = ImageSchema.readImages(\"./data/mnist/training/0\").withColumn(\"label\", lit(0))\n",
    "#one_df = ImageSchema.readImages(\"./data/mnist/training/1\").withColumn(\"label\", lit(1))\n",
    "#two_df = ImageSchema.readImages(\"./data/mnist/training/2\").withColumn(\"label\", lit(2))\n",
    "#three_df = ImageSchema.readImages(\"./data/mnist/training/3\").withColumn(\"label\", lit(3))\n",
    "#four_df = ImageSchema.readImages(\"./data/mnist/training/4\").withColumn(\"label\", lit(4))\n",
    "#five_df = ImageSchema.readImages(\"./data/mnist/training/5\").withColumn(\"label\", lit(5))\n",
    "#six_df = ImageSchema.readImages(\"./data/mnist/training/6\").withColumn(\"label\", lit(6))\n",
    "#seven_df = ImageSchema.readImages(\"./data/mnist/training/7\").withColumn(\"label\", lit(7))\n",
    "#eight_df = ImageSchema.readImages(\"./data/mnist/training/8\").withColumn(\"label\", lit(8))\n",
    "#nine_df = ImageSchema.readImages(\"./data/mnist/training/9\").withColumn(\"label\", lit(9))\n",
    "\n",
    "\n",
    "# merge train data frame\n",
    "from functools import reduce\n",
    "\n",
    "dataframes = [zero_df]#, one_df, two_df, three_df, \n",
    "              #four_df,five_df,six_df,seven_df,eight_df,nine_df]\n",
    "\n",
    "df = reduce(lambda first, second: first.union(second), dataframes)\n",
    "\n",
    "# repartition dataframe \n",
    "df = df.repartition(200)\n",
    "train = df\n",
    "\n",
    "# # On hot encoding \n",
    "# from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "# encoder = OneHotEncoderEstimator(inputCols=[\"label\"],outputCols=[\"one_hot_label\"])\n",
    "# model = encoder.fit(df)\n",
    "# df = model.transform(df)\n",
    "\n",
    "# split the data-frame\n",
    "#train, test = df.randomSplit([0.8, 0.2], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o281.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 3776, 172.18.0.6, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-18cef791d4fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzero_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2143\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o281.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 3776, 172.18.0.6, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "zero_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(file:///home/jovyan/work/data/mnist/training/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(file:///home/jovyan/work/data/mnist/training/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(file:///home/jovyan/work/data/mnist/training/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(file:///home/jovyan/work/data/mnist/training/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(file:///home/jovyan/work/data/mnist/training/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label\n",
       "0  (file:///home/jovyan/work/data/mnist/training/...      0\n",
       "1  (file:///home/jovyan/work/data/mnist/training/...      0\n",
       "2  (file:///home/jovyan/work/data/mnist/training/...      0\n",
       "3  (file:///home/jovyan/work/data/mnist/training/...      0\n",
       "4  (file:///home/jovyan/work/data/mnist/training/...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T05:46:31.780295Z",
     "start_time": "2019-07-02T05:46:27.594454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T05:46:41.282833Z",
     "start_time": "2019-07-02T05:46:41.270447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training | Transfer Learning with Spark\n",
    "\n",
    "**Deep Learning Pipelines** enables fast transfer learning with the concept of a Featurizer.\n",
    "\n",
    "Here we combine the **InceptionV3** model and **logistic regression** in Spark. The **DeepImageFeaturizer** automatically peels off the last layer of a pre-trained neural network and uses the output from all the previous layers as features for the logistic regression algorithm. \n",
    "\n",
    "Since logistic regression is a simple and fast algorithm, this transfer learning training can converge quickly using far fewer images than are typically required to train a deep learning model from ground-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n",
      "WARNING:tensorflow:From /opt/conda/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-14 20:29:10,035 WARNING (MainThread-23960) From /opt/conda/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.2114 - accuracy: 0.9369\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 14s 242us/step - loss: 0.0848 - accuracy: 0.9736\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0587 - accuracy: 0.9816\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 15s 242us/step - loss: 0.0447 - accuracy: 0.9849\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0363 - accuracy: 0.9879\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0286 - accuracy: 0.9904\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 15s 242us/step - loss: 0.0263 - accuracy: 0.9910\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 0.0219 - accuracy: 0.9924\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 0.0207 - accuracy: 0.9928\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0176 - accuracy: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f18dfccf470>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "image_index = 7777 # You may select anything up to 60,000\n",
    "\n",
    "print(y_train[image_index]) # The label is 8\n",
    "plt.imshow(x_train[image_index], cmap='Greys')\n",
    "\n",
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=x_train,y=y_train, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 86us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06560795710320767, 0.9855999946594238]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)\n",
    "#extract = Model(model.inputs, model.layers[-3]) # Dense(128,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T05:50:14.191187Z",
     "start_time": "2019-07-02T05:47:05.810834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 376 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-14 19:26:38,137 INFO (MainThread-8673) Froze 376 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted 376 variables to const ops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-14 19:26:38,282 INFO (MainThread-8673) Converted 376 variables to const ops.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o194.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 3765, 172.18.0.4, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/sparkdl/transformers/named_image.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    156\u001b[0m                                              \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                                              modelName=self.getModelName(), featurize=True)\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/sparkdl/transformers/named_image.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    219\u001b[0m                                            outputMode=modelGraphSpec[\"outputMode\"])\n\u001b[1;32m    220\u001b[0m         \u001b[0mresizeUdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresizeImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelGraphSpec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inputTensorSize\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresizedCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresizeUdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresizedCol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/sparkdl/transformers/tf_image.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mcomposed_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_addReshapeLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getImageDtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mfinal_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stripGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomposed_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py35/lib/python3.5/site-packages/sparkdl/transformers/tf_image.py\u001b[0m in \u001b[0;36m_getImageDtype\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# This may not be the best way to get the type of image, but it is one way.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# Assumes that the dtype for all images is the same in the given dataframe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mimg_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparkModeLookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \"\"\"\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o194.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 3765, 172.18.0.4, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 366, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 241, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 168, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'sparkdl'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer \n",
    "\n",
    "# model: InceptionV3\n",
    "# extracting feature from images\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n",
    "\n",
    "# used as a multi class classifier\n",
    "lr = LogisticRegression(maxIter=5, regParam=0.03, \n",
    "                        elasticNetParam=0.5, labelCol=\"label\") \n",
    "\n",
    "# define a pipeline model\n",
    "sparkdn = Pipeline(stages=[featurizer, lr])\n",
    "spark_model = sparkdn.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Now, it's time to evaluate the model performance. We now like to evaluate four evaluation matrics such as F1-score, Precision, Recall, Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T05:51:56.873137Z",
     "start_time": "2019-07-02T05:51:55.453578Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# evaluate the model with test set\n",
    "evaluator = MulticlassClassificationEvaluator() \n",
    "transform_test = spark_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T05:57:50.612018Z",
     "start_time": "2019-07-02T05:52:06.213160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score  0.8111782234361806\n",
      "Precision  0.8422058244785519\n",
      "Recall  0.8090909090909091\n",
      "Accuracy  0.8090909090909091\n"
     ]
    }
   ],
   "source": [
    "print('F1-Score ', evaluator.evaluate(transform_test, \n",
    "                                      {evaluator.metricName: 'f1'}))\n",
    "print('Precision ', evaluator.evaluate(transform_test,\n",
    "                                       {evaluator.metricName: 'weightedPrecision'}))\n",
    "print('Recall ', evaluator.evaluate(transform_test, \n",
    "                                    {evaluator.metricName: 'weightedRecall'}))\n",
    "print('Accuracy ', evaluator.evaluate(transform_test, \n",
    "                                      {evaluator.metricName: 'accuracy'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Metrix\n",
    "\n",
    " Here, we'll summarize the performance of a classification model using the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T05:58:10.111388Z",
     "start_time": "2019-07-02T05:58:08.282151Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.GnBu):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T06:02:45.798452Z",
     "start_time": "2019-07-02T06:01:33.984545Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "- Convert Spark-DataFrame to Pnadas-DataFrame\n",
    "- Call Confusion Matrix With 'True' and 'Predicted' Label\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = transform_test.select(\"label\")\n",
    "y_true = y_true.toPandas() # convert to pandas dataframe from spark dataframe\n",
    "\n",
    "y_pred = transform_test.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas() # convert to pandas dataframe from spark dataframe\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred,labels=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T06:02:55.173429Z",
     "start_time": "2019-07-02T06:02:52.954886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "- Visualize the 'Confusion Matrix' \n",
    "'''\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.grid(False)\n",
    "\n",
    "# call pre defined function\n",
    "plot_confusion_matrix(cnf_matrix, classes=range(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "Here we can also get report on classification of each class by the evaluation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T06:03:51.267355Z",
     "start_time": "2019-07-02T06:03:51.239694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.92      0.96        13\n",
      "     Class 1       0.57      1.00      0.73         8\n",
      "     Class 2       0.64      1.00      0.78         7\n",
      "     Class 3       0.88      0.70      0.78        10\n",
      "     Class 4       0.90      1.00      0.95         9\n",
      "     Class 5       0.67      0.83      0.74        12\n",
      "     Class 6       0.83      0.62      0.71         8\n",
      "     Class 7       1.00      0.80      0.89        10\n",
      "     Class 8       1.00      0.80      0.89        20\n",
      "     Class 9       0.70      0.54      0.61        13\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       110\n",
      "   macro avg       0.82      0.82      0.80       110\n",
      "weighted avg       0.84      0.81      0.81       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Classification Report of each class group\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = [\"Class {}\".format(i) for i in range(10)]\n",
    "print(classification_report(y_true, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T06:04:06.330136Z",
     "start_time": "2019-07-02T06:03:57.598328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 110)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC Score\n",
    "\n",
    "Let's also find ROC AUC socre point of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T06:05:19.066787Z",
     "start_time": "2019-07-02T06:05:19.039588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.9005280079560635\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- A custom ROC AUC score function for multi-class classification problem\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "\n",
    "print('ROC AUC score:', multiclass_roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Samples\n",
    "\n",
    "Using though a imagenet weight such model really perform pretty promising to recognise handwritten digits. Moreover we didn't also perform any image processing task to better generalization. Also model being trained on very few amounts of data compared to ImageNet dataset where such model is quite fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T06:07:21.630779Z",
     "start_time": "2019-07-02T06:07:12.265152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'label', 'features', 'rawPrediction', 'probability', 'prediction']\n",
      "+--------------------+----------+-----+\n",
      "|               image|prediction|label|\n",
      "+--------------------+----------+-----+\n",
      "|[file:/home/innat...|       1.0|    1|\n",
      "|[file:/home/innat...|       8.0|    8|\n",
      "|[file:/home/innat...|       9.0|    9|\n",
      "|[file:/home/innat...|       1.0|    8|\n",
      "|[file:/home/innat...|       1.0|    1|\n",
      "|[file:/home/innat...|       1.0|    9|\n",
      "|[file:/home/innat...|       0.0|    0|\n",
      "|[file:/home/innat...|       2.0|    9|\n",
      "|[file:/home/innat...|       8.0|    8|\n",
      "|[file:/home/innat...|       9.0|    9|\n",
      "|[file:/home/innat...|       0.0|    0|\n",
      "|[file:/home/innat...|       4.0|    0|\n",
      "|[file:/home/innat...|       5.0|    9|\n",
      "|[file:/home/innat...|       1.0|    1|\n",
      "|[file:/home/innat...|       9.0|    9|\n",
      "|[file:/home/innat...|       9.0|    9|\n",
      "|[file:/home/innat...|       1.0|    1|\n",
      "|[file:/home/innat...|       1.0|    1|\n",
      "|[file:/home/innat...|       9.0|    9|\n",
      "|[file:/home/innat...|       3.0|    6|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Comparing true vs predicted samples\n",
    "'''\n",
    "\n",
    "# all columns after transformations\n",
    "print(transform_test.columns)\n",
    "\n",
    "# see some predicted output\n",
    "transform_test.select('image', \"prediction\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 pour Spark",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
