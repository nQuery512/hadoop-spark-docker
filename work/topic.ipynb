{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r 'requirements.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "import pandas as pd\n",
    "import re as re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer , IDF, StopWordsRemover\n",
    "#from pyspark.mllib.linalg import Vector, Vectors as MLlibVectors\n",
    "from pyspark.sql.functions import udf, col, size, explode, regexp_replace, trim, lower, lit\n",
    "#from pyspark.mllib.clustering import LDA as MLlibLDA\n",
    "\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=pyspark.SparkConf().setAppName(\"TEST\").setMaster(\"spark://54.72.254.85:7077\")\n",
    "conf.set(\"spark.executor.memory\", \"3g\")\n",
    "conf.set(\"spark.driver.memory\", \"3g\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = pyspark.sql.SQLContext(sc)\n",
    "\n",
    "# reading the data\n",
    "data = sqlContext.read.format(\"csv\") \\\n",
    "   .options(header='true', inferschema='true') \\\n",
    "   .load(os.path.realpath(\"data/imdb_master.csv\"))\n",
    "\n",
    "print(data.count())\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews = data.rdd.map(lambda x : x['review']).filter(lambda x: x is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "tokens = reviews                                                   \\\n",
    "    .map( lambda document: document.strip().lower())               \\\n",
    "    .map( lambda document: re.split(\" \", document))                \\\n",
    "    .map( lambda word: [x for x in word if x.isalpha()])           \\\n",
    "    .map( lambda word: [x for x in word if len(x) > 3] )           \\\n",
    "    .map( lambda word: [x for x in word if x not in StopWords])    \\\n",
    "    .zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txts = sqlContext.createDataFrame(tokens, [\"list_of_words\",'index'])\n",
    "\n",
    "# TF\n",
    "cv = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"raw_features\", vocabSize=5000, minDF=10.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "\n",
    "result_cv = cvmodel.transform(df_txts)\n",
    "result_cv.show()\n",
    "\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_txts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "max_iterations = 100\n",
    "\n",
    "toTrain = result_tfidf[['index','features']].rdd.mapValues(MLlibVectors.fromML).map(list)\n",
    "lda_model = MLlibLDA.train(toTrain, k=num_topics, maxIterations=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordNumbers = 10 \n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = wordNumbers))\n",
    "vocabArray = cvmodel.vocabulary\n",
    "def topic_render(topic):\n",
    "    terms = topic[0]\n",
    "    result = []\n",
    "    for i in range(wordNumbers):\n",
    "        term = vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "\n",
    "topics_final = topicIndices.map(lambda topic: topic_render(topic)).collect()\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic\" + str(topic) + \":\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=num_topics, maxIter=max_iterations)\n",
    "model = lda.fit(result_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save(\"saved_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(result_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_to_pyldavis(df_filtered, count_vectorizer, transformed, lda_model):\n",
    "    xxx = df_filtered.select((explode(df_filtered.words_filtered)).alias(\"words\")).groupby(\"words\").count()\n",
    "    word_counts = {r['words']:r['count'] for r in xxx.collect()}\n",
    "    word_counts = cvmodel.vocabulary\n",
    "    #word_counts = [word_counts[w] for w in count_vectorizer.vocabulary]\n",
    "\n",
    "\n",
    "    data = {'topic_term_dists': np.array(lda_model.topicsMatrix().toArray()).T, \n",
    "            'doc_topic_dists': np.array([x.toArray() for x in transformed.select([\"topicDistribution\"]).toPandas()['topicDistribution']]),\n",
    "            'doc_lengths': [r[0] for r in df_filtered.select(size(df_filtered.words_filtered)).collect()],\n",
    "            'vocab': count_vectorizer.vocabulary,\n",
    "            'term_frequency': word_counts}\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_bad_docs(data):\n",
    "    bad = 0\n",
    "    doc_topic_dists_filtrado = []\n",
    "    doc_lengths_filtrado = []\n",
    "\n",
    "    for x,y in zip(data['doc_topic_dists'], data['doc_lengths']):\n",
    "        if np.sum(x)==0:\n",
    "            bad+=1\n",
    "        elif np.sum(x) != 1:\n",
    "            bad+=1\n",
    "        elif np.isnan(x).any():\n",
    "            bad+=1\n",
    "        else:\n",
    "            doc_topic_dists_filtrado.append(x)\n",
    "            doc_lengths_filtrado.append(y)\n",
    "\n",
    "    data['doc_topic_dists'] = doc_topic_dists_filtrado\n",
    "    data['doc_lengths'] = doc_lengths_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT DATA AND PASS IT TO PYLDAVIS\n",
    "remover = StopWordsRemover(inputCol=\"list_of_words\", outputCol=\"words_filtered\")\n",
    "df_txts = remover.transform(df_txts)#.show(truncate=False)\n",
    "df_txts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = format_data_to_pyldavis(df_txts, cvmodel, transformed, model)\n",
    "filter_bad_docs(data) # this is, because for some reason some docs apears with 0 value in all the vectors, or the norm is not 1, so I filter those docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_lda_prepared_data = pyLDAvis.prepare(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(py_lda_prepared_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 pour Spark",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
